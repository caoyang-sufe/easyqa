{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#测试dataset.yield_batch\" data-toc-modified-id=\"测试dataset.yield_batch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>测试dataset.yield_batch</a></span></li><li><span><a href=\"#测试model.generate_model_inputs\" data-toc-modified-id=\"测试model.generate_model_inputs-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>测试model.generate_model_inputs</a></span></li><li><span><a href=\"#测试pipeline.easy_inference_pipeline\" data-toc-modified-id=\"测试pipeline.easy_inference_pipeline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>测试pipeline.easy_inference_pipeline</a></span></li><li><span><a href=\"#测试modules\" data-toc-modified-id=\"测试modules-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>测试modules</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T12:45:58.315369Z",
     "start_time": "2024-10-11T12:45:58.286234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: D:\\code\\python\\project\\caoyang\\project_019_llm_reasoning\\easyqa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 切换工作目录\n",
    "if not \"CHDIR_FLAG\" in dir():\n",
    "    os.chdir(\"../\")\n",
    "    CHDIR_FLAG = True\n",
    "else:\n",
    "    assert CHDIR_FLAG is True, CHDIR_FLAG\n",
    "\n",
    "# 导入必要的包\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from configs import ModuleConfig\n",
    "from settings import DATA_DIR, LOG_DIR, MODEL_ROOT, DATA_SUMMARY, MODEL_SUMMARY\n",
    "\n",
    "from src.datasets import RaceDataset, DreamDataset, SquadDataset, HotpotqaDataset, MusiqueDataset, TriviaqaDataset\n",
    "from src.models import RobertaLargeFinetunedRace, LongformerLarge4096AnsweringRace, RobertaBaseSquad2, Chatglm6bInt4, Chatglm26bInt4\n",
    "from src.pipelines import RacePipeline, DreamPipeline, SquadPipeline\n",
    "from src.tools.easy import initialize_logger, terminate_logger, load_args\n",
    "from src.modules import CoMatch\n",
    "\n",
    "print(f\"当前工作目录: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试dataset.yield_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T12:50:28.711653Z",
     "start_time": "2024-10-08T12:48:32.725595Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_yield_batch():\n",
    "    # data_dir = r\"D:\\data\"\t# Lab PC\n",
    "    # data_dir = r\"D:\\resource\\data\"\t# Region Laptop\n",
    "    data_dir = DATA_DIR\t# default\n",
    "    data_dir_race = DATA_SUMMARY[\"RACE\"][\"path\"]\n",
    "    data_dir_dream = DATA_SUMMARY[\"DREAM\"][\"path\"]\n",
    "    data_dir_squad = DATA_SUMMARY[\"SQuAD\"][\"path\"]\n",
    "    data_dir_hotpotqa = DATA_SUMMARY[\"HotpotQA\"][\"path\"]\n",
    "    data_dir_musique = DATA_SUMMARY[\"Musique\"][\"path\"]\n",
    "    data_dir_triviaqa = DATA_SUMMARY[\"TriviaQA\"][\"path\"]\n",
    "\n",
    "    # RACE\n",
    "    def _test_race():\n",
    "        print(_test_race.__name__)\n",
    "        dataset = RaceDataset(data_dir=data_dir_race)\n",
    "        for batch in dataset.yield_batch(batch_size=2, types=[\"train\", \"dev\"], difficulties=[\"high\"]):\n",
    "            pass\n",
    "    # DREAM\n",
    "    def _test_dream():\n",
    "        print(_test_dream.__name__)\n",
    "        dataset = DreamDataset(data_dir=data_dir_dream)\n",
    "        for batch in dataset.yield_batch(batch_size=2, types=[\"train\", \"dev\"]):\n",
    "            pass\n",
    "    # SQuAD\n",
    "    def _test_squad():\n",
    "        print(_test_squad.__name__)\n",
    "        dataset = SquadDataset(data_dir=data_dir_squad)\n",
    "        versions = [\"1.1\"]\n",
    "        types = [\"train\", \"dev\"]\n",
    "        for version in versions:\n",
    "            for type_ in types:\n",
    "                for i, batch in enumerate(dataset.yield_batch(batch_size=2, type_=type_, version=version)):\n",
    "                    if i > 5:\n",
    "                        break\n",
    "                    print(batch)\n",
    "    # HotpotQA\n",
    "    def _test_hotpotqa():\n",
    "        print(_test_hotpotqa.__name__)\n",
    "        dataset = HotpotqaDataset(data_dir=data_dir_hotpotqa)\n",
    "        filenames = [\"hotpot_train_v1.1.json\",\n",
    "                     \"hotpot_dev_distractor_v1.json\",\n",
    "                     \"hotpot_dev_fullwiki_v1.json\",\n",
    "                     \"hotpot_test_fullwiki_v1.json\",\n",
    "                     ]\n",
    "        for filename in filenames:\n",
    "            for i, batch in enumerate(dataset.yield_batch(batch_size=2, filename=filename)):\n",
    "                if i > 5:\n",
    "                    break\n",
    "                print(batch)\n",
    "    # Musique\n",
    "    def _test_musique():\n",
    "        print(_test_musique.__name__)\n",
    "        batch_size = 2\n",
    "        dataset = MusiqueDataset(data_dir=data_dir_musique)\n",
    "        types = [\"train\", \"dev\", \"test\"]\n",
    "        categories = [\"ans\", \"full\"]\n",
    "        answerables = [True, False]\n",
    "        for type_ in types:\n",
    "            for category in categories:\n",
    "                if category == \"full\":\n",
    "                    for answerable in answerables:\n",
    "                        print(f\"======== {type_} - {category} - {answerable} ========\")\n",
    "                        for i, batch in enumerate(dataset.yield_batch(batch_size, type_, category, answerable)):\n",
    "                            if i > 5:\n",
    "                                break\n",
    "                            print(batch)\n",
    "                else:\n",
    "                    print(f\"======== {type_} - {category} ========\")\n",
    "                    for i, batch in enumerate(dataset.yield_batch(batch_size, type_, category)):\n",
    "                        if i > 5:\n",
    "                            break\n",
    "                        print(batch)\n",
    "\n",
    "    # TriviaQA\n",
    "    def _test_triviaqa():\n",
    "        print(_test_triviaqa.__name__)\n",
    "        n = 1\n",
    "        batch_size = 2\n",
    "        dataset = TriviaqaDataset(data_dir=data_dir_triviaqa)\n",
    "        types = [\"verified\", \"train\", \"dev\", \"test\"]\n",
    "        categories = [\"web\", \"wikipedia\"]\n",
    "        for type_ in types:\n",
    "            for category in categories:\n",
    "                print(f\"======== {type_} - {category} ========\")\n",
    "                for i, batch in enumerate(dataset.yield_batch(batch_size, type_, category, False)):\n",
    "                    if i > n:\n",
    "                        break\n",
    "                    print(batch[0][\"question\"], batch[0][\"answers\"])\n",
    "        gc.collect()\n",
    "        for type_ in types[1:]:\n",
    "            print(f\"======== {type_} - unfiltered ========\")\n",
    "            for i, batch in enumerate(dataset.yield_batch(batch_size, type_, \"web\", True)):\n",
    "                if i > n:\n",
    "                    break\n",
    "                print(batch[0][\"question\"], batch[0][\"answers\"])\n",
    "\n",
    "    # Test\n",
    "    logger = initialize_logger(os.path.join(LOG_DIR, \"sanity.log\"), 'w')\n",
    "#     _test_race()\n",
    "#     _test_dream()\n",
    "#     _test_squad()\n",
    "#     _test_hotpotqa()\n",
    "#     _test_musique()\n",
    "    _test_triviaqa()\n",
    "    terminate_logger(logger)\n",
    "\n",
    "test_yield_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试model.generate_model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T12:54:10.485101Z",
     "start_time": "2024-10-08T12:54:08.592621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 20:54:08,605 | base.py | INFO | Check data directory: D:\\resource\\data\\RACE\n",
      "2024-10-08 20:54:08,606 | base.py | INFO | √ ./train/high/\n",
      "2024-10-08 20:54:08,607 | base.py | INFO | √ ./train/middle/\n",
      "2024-10-08 20:54:08,607 | base.py | INFO | √ ./dev/high/\n",
      "2024-10-08 20:54:08,607 | base.py | INFO | √ ./dev/middle/\n",
      "2024-10-08 20:54:08,607 | base.py | INFO | √ ./test/high/\n",
      "2024-10-08 20:54:08,607 | base.py | INFO | √ ./test/middle/\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_test_race\n",
      "{'input_ids': tensor([[[    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,   133,  1623,  3829,\n",
      "           3482,   142,  1437,  1437,    37,    34,   203,   418,     4,  1437,\n",
      "            479,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,   133,  1623,  3829,\n",
      "           3482,   142,  1437,  1437,    37,  3829,     5,  6464,     4,  1437,\n",
      "            479,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,   133,  1623,  3829,\n",
      "           3482,   142,  1437,  1437,    37,  3829,     7,  8933,     5,   850,\n",
      "            227,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,   133,  1623,  3829,\n",
      "           3482,   142,  1437,  1437,    37,    34,  1085,     7,   109,    53,\n",
      "           3482,     2]],\n",
      "\n",
      "        [[    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,  1213,   393,   213,\n",
      "           3482,   561,   142,  1437,    49,  1319,     9,  3482,    32,  1341,\n",
      "            430,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,  1213,   393,   213,\n",
      "           3482,   561,   142,  1437,    51,  4157,   349,    97,     4,  1437,\n",
      "            479,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,  1213,   393,   213,\n",
      "           3482,   561,   142,  1437,    51,   240,   282,    75,   907,   932,\n",
      "             13,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,  1213,   393,   213,\n",
      "           3482,   561,   142,  1437,    51,   218,    75,    33,    86,    13,\n",
      "             24,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,   133,  1623,  3829,\n",
      "           3482,   142,  1437,  1437,    37,    34,   203,   418,     4,  1437,\n",
      "            479,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,   133,  1623,  3829,\n",
      "           3482,   142,  1437,  1437,    37,  3829,     5,  6464,     4,  1437,\n",
      "            479,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,   133,  1623,  3829,\n",
      "           3482,   142,  1437,  1437,    37,  3829,     7,  8933,     5,   850,\n",
      "            227,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,   133,  1623,  3829,\n",
      "           3482,   142,  1437,  1437,    37,    34,  1085,     7,   109,    53,\n",
      "           3482,     2]],\n",
      "\n",
      "        [[    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,  1213,   393,   213,\n",
      "           3482,   561,   142,  1437,    49,  1319,     9,  3482,    32,  1341,\n",
      "            430,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,  1213,   393,   213,\n",
      "           3482,   561,   142,  1437,    51,  4157,   349,    97,     4,  1437,\n",
      "            479,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,  1213,   393,   213,\n",
      "           3482,   561,   142,  1437,    51,   240,   282,    75,   907,   932,\n",
      "             13,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2,  1213,   393,   213,\n",
      "           3482,   561,   142,  1437,    51,   218,    75,    33,    86,    13,\n",
      "             24,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,    64,    75,\n",
      "            109,     5,  3482,   157,   142,  1437,  1437,    37,    16,   664,\n",
      "           1437,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,    64,    75,\n",
      "            109,     5,  3482,   157,   142,  1437,  1437,    37,    16, 11640,\n",
      "             12,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,    64,    75,\n",
      "            109,     5,  3482,   157,   142,  1437,  1437,    37,   747, 13585,\n",
      "             39,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,    64,    75,\n",
      "            109,     5,  3482,   157,   142,  1437,  1437,    37,   630,    75,\n",
      "            101,     2]],\n",
      "\n",
      "        [[    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,   399,    75,\n",
      "            907,    99,    39,   985,   770,   142,  1437,     5,  2792,    21,\n",
      "           1367,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,   399,    75,\n",
      "            907,    99,    39,   985,   770,   142,  1437,     5, 20976,  2294,\n",
      "            123,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,   399,    75,\n",
      "            907,    99,    39,   985,   770,   142,  1437,    37, 18774,   103,\n",
      "              9,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,   399,    75,\n",
      "            907,    99,    39,   985,   770,   142,  1437,    37,   851,    70,\n",
      "              5,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,    64,    75,\n",
      "            109,     5,  3482,   157,   142,  1437,  1437,    37,    16,   664,\n",
      "           1437,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,    64,    75,\n",
      "            109,     5,  3482,   157,   142,  1437,  1437,    37,    16, 11640,\n",
      "             12,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,    64,    75,\n",
      "            109,     5,  3482,   157,   142,  1437,  1437,    37,   747, 13585,\n",
      "             39,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,    64,    75,\n",
      "            109,     5,  3482,   157,   142,  1437,  1437,    37,   630,    75,\n",
      "            101,     2]],\n",
      "\n",
      "        [[    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,   399,    75,\n",
      "            907,    99,    39,   985,   770,   142,  1437,     5,  2792,    21,\n",
      "           1367,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,   399,    75,\n",
      "            907,    99,    39,   985,   770,   142,  1437,     5, 20976,  2294,\n",
      "            123,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,   399,    75,\n",
      "            907,    99,    39,   985,   770,   142,  1437,    37, 18774,   103,\n",
      "              9,     2],\n",
      "         [    0,  2387,  1623,    16,    10,  2421, 14172,  5961,     4,    91,\n",
      "           6138,     7,   356,    23,   383,     2,     2, 35693,   399,    75,\n",
      "            907,    99,    39,   985,   770,   142,  1437,    37,   851,    70,\n",
      "              5,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 32251,     9,     5,\n",
      "            511,    16,  1528,     9,     5,  7740,     9,  6845,    88,  1444,\n",
      "            116,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 32251,     9,     5,\n",
      "            511,    16,  1528,     9,     5,  7740,     9,  6845,    88,  1444,\n",
      "            116,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 32251,     9,     5,\n",
      "            511,    16,  1528,     9,     5,  7740,     9,  6845,    88,  1444,\n",
      "            116,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 32251,     9,     5,\n",
      "            511,    16,  1528,     9,     5,  7740,     9,  6845,    88,  1444,\n",
      "            116,     2]],\n",
      "\n",
      "        [[    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,   713,  9078,  4412,\n",
      "          14524,  1437,     5,   750,     9,  6845,  4835,    11,  1444,  1437,\n",
      "            479,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,   713,  9078,  4412,\n",
      "          14524,  1437,   141,  6845,  1059,    10,  1406,  4076,    11,  1444,\n",
      "           1437,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,   713,  9078,  4412,\n",
      "          14524,  1437,   141,     5, 24226,   300,     5, 10870,     9,  4835,\n",
      "           6845,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,  1005,     2,     2,   713,  9078,\n",
      "           4412, 14524,  1437,   141,  6845,    12,   958,    21,  2421,  1437,\n",
      "            479,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 32251,     9,     5,\n",
      "            511,    16,  1528,     9,     5,  7740,     9,  6845,    88,  1444,\n",
      "            116,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 32251,     9,     5,\n",
      "            511,    16,  1528,     9,     5,  7740,     9,  6845,    88,  1444,\n",
      "            116,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 32251,     9,     5,\n",
      "            511,    16,  1528,     9,     5,  7740,     9,  6845,    88,  1444,\n",
      "            116,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 32251,     9,     5,\n",
      "            511,    16,  1528,     9,     5,  7740,     9,  6845,    88,  1444,\n",
      "            116,     2]],\n",
      "\n",
      "        [[    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,   713,  9078,  4412,\n",
      "          14524,  1437,     5,   750,     9,  6845,  4835,    11,  1444,  1437,\n",
      "            479,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,   713,  9078,  4412,\n",
      "          14524,  1437,   141,  6845,  1059,    10,  1406,  4076,    11,  1444,\n",
      "           1437,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,   713,  9078,  4412,\n",
      "          14524,  1437,   141,     5, 24226,   300,     5, 10870,     9,  4835,\n",
      "           6845,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,  1005,     2,     2,   713,  9078,\n",
      "           4412, 14524,  1437,   141,  6845,    12,   958,    21,  2421,  1437,\n",
      "            479,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,  1005,     2,     2, 45866,  1059,\n",
      "             10,  1406,  4076,    11,  1444,  1437,    11, 46111,  3220,  1437,\n",
      "            479,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 45866,  1059,    10,\n",
      "           1406,  4076,    11,  1444,  1437,    11,   411, 33326,  3220,  1437,\n",
      "            479,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 45866,  1059,    10,\n",
      "           1406,  4076,    11,  1444,  1437,    11, 42081, 43169,  3220,  1437,\n",
      "            479,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 45866,  1059,    10,\n",
      "           1406,  4076,    11,  1444,  1437,    11,     5,   628, 42081, 43169,\n",
      "           3220,     2]],\n",
      "\n",
      "        [[    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,  4763,    11,  1005,\n",
      "            880,     7,  4076,  6845,    19,  5803,   142,  1437,    24, 29143,\n",
      "            101,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,  4763,    11,  1005,\n",
      "            880,     7,  4076,  6845,    19,  5803,   142,  1437,    24, 29143,\n",
      "             55,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,  4763,    11,  1005,\n",
      "            880,     7,  4076,  6845,    19,  5803,   142,  1437,    24,  1059,\n",
      "             10,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,  4763,    11,  1005,\n",
      "            880,     7,  4076,  6845,    19,  5803,   142,  1437, 38367,   263,\n",
      "          15206,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,  1005,     2,     2, 45866,  1059,\n",
      "             10,  1406,  4076,    11,  1444,  1437,    11, 46111,  3220,  1437,\n",
      "            479,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 45866,  1059,    10,\n",
      "           1406,  4076,    11,  1444,  1437,    11,   411, 33326,  3220,  1437,\n",
      "            479,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 45866,  1059,    10,\n",
      "           1406,  4076,    11,  1444,  1437,    11, 42081, 43169,  3220,  1437,\n",
      "            479,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2, 45866,  1059,    10,\n",
      "           1406,  4076,    11,  1444,  1437,    11,     5,   628, 42081, 43169,\n",
      "           3220,     2]],\n",
      "\n",
      "        [[    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,  4763,    11,  1005,\n",
      "            880,     7,  4076,  6845,    19,  5803,   142,  1437,    24, 29143,\n",
      "            101,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,  4763,    11,  1005,\n",
      "            880,     7,  4076,  6845,    19,  5803,   142,  1437,    24, 29143,\n",
      "             55,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,  4763,    11,  1005,\n",
      "            880,     7,  4076,  6845,    19,  5803,   142,  1437,    24,  1059,\n",
      "             10,     2],\n",
      "         [    0, 45866,  4835,    21,  1537,    11,   436,    13,   823,    65,\n",
      "           7673,   107,   137,  1268,    11,     2,     2,  4763,    11,  1005,\n",
      "            880,     7,  4076,  6845,    19,  5803,   142,  1437, 38367,   263,\n",
      "          15206,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2765, 16407,   514,\n",
      "             88,     5,  4946,     6,     5, 31887,   144,  1153,   770,     5,\n",
      "          20875,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2765, 16407,   514,\n",
      "             88,     5,  4946,     6,     5, 31887,   144,  1153,   770,     5,\n",
      "          20875,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2765, 16407,   514,\n",
      "             88,     5,  4946,     6,     5, 31887,   144,  1153,   770,     5,\n",
      "          20875,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2765, 16407,   514,\n",
      "             88,     5,  4946,     6,     5, 31887,   144,  1153,   770,     5,\n",
      "          20875,     2]],\n",
      "\n",
      "        [[    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,   170,  1532,    14,\n",
      "           1118,     7,  1791,     6, 15179,  1437,   173,   203,  4851,  1437,\n",
      "            479,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,   170,  1532,    14,\n",
      "           1118,     7,  1791,     6, 15179,  1437,   216,   540,    59,    49,\n",
      "           2148,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,   170,  1532,    14,\n",
      "           1118,     7,  1791,     6, 15179,  1437, 10628,    55,    86,    13,\n",
      "           1235,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,   170,  1532,    14,\n",
      "           1118,     7,  1791,     6, 15179,  1437,  1930,   540,    86,    19,\n",
      "             49,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2765, 16407,   514,\n",
      "             88,     5,  4946,     6,     5, 31887,   144,  1153,   770,     5,\n",
      "          20875,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2765, 16407,   514,\n",
      "             88,     5,  4946,     6,     5, 31887,   144,  1153,   770,     5,\n",
      "          20875,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2765, 16407,   514,\n",
      "             88,     5,  4946,     6,     5, 31887,   144,  1153,   770,     5,\n",
      "          20875,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2765, 16407,   514,\n",
      "             88,     5,  4946,     6,     5, 31887,   144,  1153,   770,     5,\n",
      "          20875,     2]],\n",
      "\n",
      "        [[    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,   170,  1532,    14,\n",
      "           1118,     7,  1791,     6, 15179,  1437,   173,   203,  4851,  1437,\n",
      "            479,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,   170,  1532,    14,\n",
      "           1118,     7,  1791,     6, 15179,  1437,   216,   540,    59,    49,\n",
      "           2148,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,   170,  1532,    14,\n",
      "           1118,     7,  1791,     6, 15179,  1437, 10628,    55,    86,    13,\n",
      "           1235,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,   170,  1532,    14,\n",
      "           1118,     7,  1791,     6, 15179,  1437,  1930,   540,    86,    19,\n",
      "             49,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2024-10-08 20:54:09,082 | base.py | INFO | Check data directory: D:\\resource\\data\\dream\\data\n",
      "2024-10-08 20:54:09,083 | base.py | INFO | √ ./train.json\n",
      "2024-10-08 20:54:09,083 | base.py | INFO | √ ./dev.json\n",
      "2024-10-08 20:54:09,084 | base.py | INFO | √ ./test.json\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    18,  1437,\n",
      "              5,  3184,     9,     5,  9078,   116, 42516,   111,   625, 36237,\n",
      "             12,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    18,  1437,\n",
      "              5,  3184,     9,     5,  9078,   116, 42295,    12,  1246,   111,\n",
      "            625,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    18,  1437,\n",
      "              5,  3184,     9,     5,  9078,   116,  3718,    12, 45260,   111,\n",
      "          46781,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    18,  1437,\n",
      "              5,  3184,     9,     5,  9078,   116, 31613,   111, 45260,   111,\n",
      "            625,     2]],\n",
      "\n",
      "        [[    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    74,    28,\n",
      "              5,   275,  1270,    13,     5,  9078,   116,    20,  1808,     9,\n",
      "            301,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,  2655,     2,     2,  2264,    74,\n",
      "             28,     5,   275,  1270,    13,     5,  9078,   116, 11714,   358,\n",
      "           2289,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    74,    28,\n",
      "              5,   275,  1270,    13,     5,  9078,   116,    20,  1808,     9,\n",
      "           4835,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    74,    28,\n",
      "              5,   275,  1270,    13,     5,  9078,   116, 41183,   110,   113,\n",
      "          21033,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    18,  1437,\n",
      "              5,  3184,     9,     5,  9078,   116, 42516,   111,   625, 36237,\n",
      "             12,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    18,  1437,\n",
      "              5,  3184,     9,     5,  9078,   116, 42295,    12,  1246,   111,\n",
      "            625,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    18,  1437,\n",
      "              5,  3184,     9,     5,  9078,   116,  3718,    12, 45260,   111,\n",
      "          46781,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    18,  1437,\n",
      "              5,  3184,     9,     5,  9078,   116, 31613,   111, 45260,   111,\n",
      "            625,     2]],\n",
      "\n",
      "        [[    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    74,    28,\n",
      "              5,   275,  1270,    13,     5,  9078,   116,    20,  1808,     9,\n",
      "            301,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,  2655,     2,     2,  2264,    74,\n",
      "             28,     5,   275,  1270,    13,     5,  9078,   116, 11714,   358,\n",
      "           2289,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    74,    28,\n",
      "              5,   275,  1270,    13,     5,  9078,   116,    20,  1808,     9,\n",
      "           4835,     2],\n",
      "         [    0, 11475,  2115,    10,    86,     6,    89,    21,    10, 20875,\n",
      "             54,   770,     7,  2364,    55,     2,     2,  2264,    74,    28,\n",
      "              5,   275,  1270,    13,     5,  9078,   116, 41183,   110,   113,\n",
      "          21033,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2,  2264,  2594,     7,\n",
      "           1972,  1440, 25527,   309,     7,     5,  9078,   116,   252,  6297,\n",
      "             31,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2,  2264,  2594,     7,\n",
      "           1972,  1440, 25527,   309,     7,     5,  9078,   116,    20,  5842,\n",
      "           4750,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2,  2264,  2594,     7,\n",
      "           1972,  1440, 25527,   309,     7,     5,  9078,   116,  5763,   747,\n",
      "            146,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2,  2264,  2594,     7,\n",
      "           1972,  1440, 25527,   309,     7,     5,  9078,   116,   252,    64,\n",
      "             75,     2]],\n",
      "\n",
      "        [[    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2, 22649,  5086,   146,\n",
      "              5,  5842,  4750,  1437,  1437,  1701,     5, 20628,     9,   103,\n",
      "            390,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2, 22649,  5086,   146,\n",
      "              5,  5842,  4750,  1437,  1437,   213,    15, 14221, 16387,    71,\n",
      "            390,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2, 22649,  5086,   146,\n",
      "              5,  5842,  4750,  1437,  1437,   766, 16387,    71,   604,  1437,\n",
      "            479,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2, 22649,  5086,   146,\n",
      "              5,  5842,  4750,  1437,  1437,   356,    13,    10,    92,  5448,\n",
      "              7,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2,  2264,  2594,     7,\n",
      "           1972,  1440, 25527,   309,     7,     5,  9078,   116,   252,  6297,\n",
      "             31,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2,  2264,  2594,     7,\n",
      "           1972,  1440, 25527,   309,     7,     5,  9078,   116,    20,  5842,\n",
      "           4750,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2,  2264,  2594,     7,\n",
      "           1972,  1440, 25527,   309,     7,     5,  9078,   116,  5763,   747,\n",
      "            146,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2,  2264,  2594,     7,\n",
      "           1972,  1440, 25527,   309,     7,     5,  9078,   116,   252,    64,\n",
      "             75,     2]],\n",
      "\n",
      "        [[    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2, 22649,  5086,   146,\n",
      "              5,  5842,  4750,  1437,  1437,  1701,     5, 20628,     9,   103,\n",
      "            390,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2, 22649,  5086,   146,\n",
      "              5,  5842,  4750,  1437,  1437,   213,    15, 14221, 16387,    71,\n",
      "            390,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2, 22649,  5086,   146,\n",
      "              5,  5842,  4750,  1437,  1437,   766, 16387,    71,   604,  1437,\n",
      "            479,     2],\n",
      "         [    0,  1779,  9911,     8,  3188,  6190,     5,  1880,  1726,    30,\n",
      "             10,  6874,  1437,  1440, 25527,     2,     2, 22649,  5086,   146,\n",
      "              5,  5842,  4750,  1437,  1437,   356,    13,    10,    92,  5448,\n",
      "              7,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "_test_dream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2024-10-08 20:54:09,336 | base.py | INFO | Check data directory: D:\\resource\\data\\SQuAD\n",
      "2024-10-08 20:54:09,336 | base.py | INFO | √ ./squad1.1/train-v1.1.json\n",
      "2024-10-08 20:54:09,336 | base.py | INFO | √ ./squad1.1/dev-v1.1.json\n",
      "2024-10-08 20:54:09,336 | base.py | INFO | √ ./squad2.0/train-v2.0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    0,   448,    35,    38,   524,  2811,  6614,   127,  7950,  1380,\n",
      "              4,    38,   524,    45,   442,     2,     2,  2264,   473,     5,\n",
      "            313,  3608,     5,   693,   109,   116, 15850,    69,  7950,  3254,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,    38,   524,  2811,  6614,   127,  7950,  1380,\n",
      "              4,    38,   524,    45,   442,     2,     2,  2264,   473,     5,\n",
      "            313,  3608,     5,   693,   109,   116,  4624,    10,    55,  2679,\n",
      "           1380,     2],\n",
      "         [    0,   448,    35,    38,   524,  2811,  6614,   127,  7950,  1380,\n",
      "              4,    38,   524,    45,   442,     2,     2,  2264,   473,     5,\n",
      "            313,  3608,     5,   693,   109,   116,  5603,    69,  7950,  1380,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            313,   206,     9,     5,   693,    18,  6836,   116,    85,    18,\n",
      "            269,     2],\n",
      "         [    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            313,   206,     9,     5,   693,    18,  6836,   116,    85,    18,\n",
      "            182,     2],\n",
      "         [    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            313,   206,     9,     5,   693,    18,  6836,   116,    85,    18,\n",
      "            357,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,   448,    35,    38,   524,  2811,  6614,   127,  7950,  1380,\n",
      "              4,    38,   524,    45,   442,     2,     2,  2264,   473,     5,\n",
      "            313,  3608,     5,   693,   109,   116, 15850,    69,  7950,  3254,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,    38,   524,  2811,  6614,   127,  7950,  1380,\n",
      "              4,    38,   524,    45,   442,     2,     2,  2264,   473,     5,\n",
      "            313,  3608,     5,   693,   109,   116,  4624,    10,    55,  2679,\n",
      "           1380,     2],\n",
      "         [    0,   448,    35,    38,   524,  2811,  6614,   127,  7950,  1380,\n",
      "              4,    38,   524,    45,   442,     2,     2,  2264,   473,     5,\n",
      "            313,  3608,     5,   693,   109,   116,  5603,    69,  7950,  1380,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            313,   206,     9,     5,   693,    18,  6836,   116,    85,    18,\n",
      "            269,     2],\n",
      "         [    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            313,   206,     9,     5,   693,    18,  6836,   116,    85,    18,\n",
      "            182,     2],\n",
      "         [    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            313,   206,     9,     5,   693,    18,  6836,   116,    85,    18,\n",
      "            357,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            693,  1394,     5,   313,     7,   109,   116, 40736,  6836,  2417,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            693,  1394,     5,   313,     7,   109,   116, 16827,    39,  1141,\n",
      "            220,     2],\n",
      "         [    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            693,  1394,     5,   313,     7,   109,   116,  6319,   103, 14532,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   448,    35,  6893,    23,     5,  1816,    15,     5,  4806,\n",
      "            328, 50118,   597,    35,  5534,     6,  4420,    79,    18,   269,\n",
      "             10,     2,     2, 13841,    32,     5,    80,  5151,   116,   497,\n",
      "            184,     2],\n",
      "         [    0,   448,    35,  6893,    23,     5,  1816,    15,     5,  4806,\n",
      "            328, 50118,   597,    35,  5534,     6,  4420,    79,    18,   269,\n",
      "              2,     2, 13841,    32,     5,    80,  5151,   116,    96,    49,\n",
      "           8171,     2],\n",
      "         [    0,   448,    35,  6893,    23,     5,  1816,    15,     5,  4806,\n",
      "            328, 50118,   597,    35,  5534,     6,  4420,    79,    18,   269,\n",
      "              2,     2, 13841,    32,     5,    80,  5151,   116,   374,     5,\n",
      "           2014,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            693,  1394,     5,   313,     7,   109,   116, 40736,  6836,  2417,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            693,  1394,     5,   313,     7,   109,   116, 16827,    39,  1141,\n",
      "            220,     2],\n",
      "         [    0,   771,    35,  2647,     6,    38,   437,  6023,   127,  6836,\n",
      "            965,    75,     7,   110,  5840,     2,     2,  2264,   473,     5,\n",
      "            693,  1394,     5,   313,     7,   109,   116,  6319,   103, 14532,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   448,    35,  6893,    23,     5,  1816,    15,     5,  4806,\n",
      "            328, 50118,   597,    35,  5534,     6,  4420,    79,    18,   269,\n",
      "             10,     2,     2, 13841,    32,     5,    80,  5151,   116,   497,\n",
      "            184,     2],\n",
      "         [    0,   448,    35,  6893,    23,     5,  1816,    15,     5,  4806,\n",
      "            328, 50118,   597,    35,  5534,     6,  4420,    79,    18,   269,\n",
      "              2,     2, 13841,    32,     5,    80,  5151,   116,    96,    49,\n",
      "           8171,     2],\n",
      "         [    0,   448,    35,  6893,    23,     5,  1816,    15,     5,  4806,\n",
      "            328, 50118,   597,    35,  5534,     6,  4420,    79,    18,   269,\n",
      "              2,     2, 13841,    32,     5,    80,  5151,   116,   374,     5,\n",
      "           2014,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0,   448,    35,  2615,    47, 20453,   162,   150,    38,   524,\n",
      "           6970,     4, 50118,   771,    35,     2,     2,  2264,   473,     5,\n",
      "            313,   236,     5,   693,     7,   109,   116,   598,   310,     5,\n",
      "          13305,     2],\n",
      "         [    0,   448,    35,  2615,    47, 20453,   162,   150,    38,   524,\n",
      "           6970,     4, 50118,   771,    35,     2,     2,  2264,   473,     5,\n",
      "            313,   236,     5,   693,     7,   109,   116,   598,   492,    10,\n",
      "            819,     2],\n",
      "         [    0,   448,    35,  2615,    47, 20453,   162,   150,    38,   524,\n",
      "           6970,     4, 50118,   771,    35,     2,     2,  2264,   473,     5,\n",
      "            313,   236,     5,   693,     7,   109,   116,   598,  7884,    10,\n",
      "           2214,     2]],\n",
      "\n",
      "        [[    0,   771,    35,    38,    64,    75,  1955,    66,    99,    18,\n",
      "           1593,    19,   127,  1183,     4,     2,     2,  2264,   473,     5,\n",
      "            313,  1266,   116,    85,    40,   185,    59,    65,   353,     7,\n",
      "           5989,     2],\n",
      "         [    0,   771,    35,    38,    64,    75,  1955,    66,    99,    18,\n",
      "           1593,    19,   127,  1183,     4,     2,     2,  2264,   473,     5,\n",
      "            313,  1266,   116,    20,  1183,    16,   117,  1181,   966, 26643,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,    38,    64,    75,  1955,    66,    99,    18,\n",
      "           1593,    19,   127,  1183,     4,     2,     2,  2264,   473,     5,\n",
      "            313,  1266,   116,    85,    16,    10,   205,  1114,     7,   489,\n",
      "              5,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,   448,    35,  2615,    47, 20453,   162,   150,    38,   524,\n",
      "           6970,     4, 50118,   771,    35,     2,     2,  2264,   473,     5,\n",
      "            313,   236,     5,   693,     7,   109,   116,   598,   310,     5,\n",
      "          13305,     2],\n",
      "         [    0,   448,    35,  2615,    47, 20453,   162,   150,    38,   524,\n",
      "           6970,     4, 50118,   771,    35,     2,     2,  2264,   473,     5,\n",
      "            313,   236,     5,   693,     7,   109,   116,   598,   492,    10,\n",
      "            819,     2],\n",
      "         [    0,   448,    35,  2615,    47, 20453,   162,   150,    38,   524,\n",
      "           6970,     4, 50118,   771,    35,     2,     2,  2264,   473,     5,\n",
      "            313,   236,     5,   693,     7,   109,   116,   598,  7884,    10,\n",
      "           2214,     2]],\n",
      "\n",
      "        [[    0,   771,    35,    38,    64,    75,  1955,    66,    99,    18,\n",
      "           1593,    19,   127,  1183,     4,     2,     2,  2264,   473,     5,\n",
      "            313,  1266,   116,    85,    40,   185,    59,    65,   353,     7,\n",
      "           5989,     2],\n",
      "         [    0,   771,    35,    38,    64,    75,  1955,    66,    99,    18,\n",
      "           1593,    19,   127,  1183,     4,     2,     2,  2264,   473,     5,\n",
      "            313,  1266,   116,    20,  1183,    16,   117,  1181,   966, 26643,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,    38,    64,    75,  1955,    66,    99,    18,\n",
      "           1593,    19,   127,  1183,     4,     2,     2,  2264,   473,     5,\n",
      "            313,  1266,   116,    85,    16,    10,   205,  1114,     7,   489,\n",
      "              5,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,   244,    47,   116, 50118,     2,\n",
      "              2, 13841,   473,     5,   693,   173,   116,   497,    10,  2737,\n",
      "              4,     2],\n",
      "         [    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,   244,    47,   116,     2,     2,\n",
      "          13841,   473,     5,   693,   173,   116,   497,    10,  1012,  1992,\n",
      "              4,     2],\n",
      "         [    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,   244,    47,   116,     2,     2,\n",
      "          13841,   473,     5,   693,   173,   116,   497,    10,  2924,   558,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,     2,     2,  7608,    16,     5,\n",
      "            313,  9889,    13,    42,   737,   116,    91,   782,    10,   157,\n",
      "             12,     2],\n",
      "         [    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,     2,     2,  7608,    16,     5,\n",
      "            313,  9889,    13,    42,   737,   116,    91,    34,   682,   685,\n",
      "            277,     2],\n",
      "         [    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,     2,     2,  7608,    16,     5,\n",
      "            313,  9889,    13,    42,   737,   116,    91,  1072,   103,   447,\n",
      "            676,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,   244,    47,   116, 50118,     2,\n",
      "              2, 13841,   473,     5,   693,   173,   116,   497,    10,  2737,\n",
      "              4,     2],\n",
      "         [    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,   244,    47,   116,     2,     2,\n",
      "          13841,   473,     5,   693,   173,   116,   497,    10,  1012,  1992,\n",
      "              4,     2],\n",
      "         [    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,   244,    47,   116,     2,     2,\n",
      "          13841,   473,     5,   693,   173,   116,   497,    10,  2924,   558,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,     2,     2,  7608,    16,     5,\n",
      "            313,  9889,    13,    42,   737,   116,    91,   782,    10,   157,\n",
      "             12,     2],\n",
      "         [    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,     2,     2,  7608,    16,     5,\n",
      "            313,  9889,    13,    42,   737,   116,    91,    34,   682,   685,\n",
      "            277,     2],\n",
      "         [    0,   771,    35, 20920,     6,    42,    16,   255,  3573, 10276,\n",
      "          11863,     4,  1336,    64,    38,     2,     2,  7608,    16,     5,\n",
      "            313,  9889,    13,    42,   737,   116,    91,  1072,   103,   447,\n",
      "            676,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0,   448,    35,    38,   206,    24,    18,   164,     7,  1895,\n",
      "              4, 50118,   771,    35,    38,  4443,    98,     4,    20,     2,\n",
      "              2,  2264,    16,     5,  1650,   101,   116,    85,    18, 31832,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,    38,   206,    24,    18,   164,     7,  1895,\n",
      "              4, 50118,   771,    35,    38,  4443,    98,     4,    20,     2,\n",
      "              2,  2264,    16,     5,  1650,   101,   116,    85,    18,  2779,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,    38,   206,    24,    18,   164,     7,  1895,\n",
      "              4, 50118,   771,    35,    38,  4443,    98,     4,    20,     2,\n",
      "              2,  2264,    16,     5,  1650,   101,   116,    85,    18,  5419,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   771,    35,  7204,  8429,     4,  2615,    38,   244,    47,\n",
      "            116, 50118,   448,    35,  8976,     6, 20280,     6,    42,     2,\n",
      "              2, 13841,    32,     5,    80,  6864,   116,    96,    10,  2391,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,  7204,  8429,     4,  2615,    38,   244,    47,\n",
      "            116, 50118,   448,    35,  8976,     6, 20280,     6,    42,     2,\n",
      "              2, 13841,    32,     5,    80,  6864,   116,    96,    10,  2303,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,  7204,  8429,     4,  2615,    38,   244,    47,\n",
      "            116, 50118,   448,    35,  8976,     6, 20280,     6,    42,     2,\n",
      "              2, 13841,    32,     5,    80,  6864,   116,    96,     5,  2014,\n",
      "              4,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,   448,    35,    38,   206,    24,    18,   164,     7,  1895,\n",
      "              4, 50118,   771,    35,    38,  4443,    98,     4,    20,     2,\n",
      "              2,  2264,    16,     5,  1650,   101,   116,    85,    18, 31832,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,    38,   206,    24,    18,   164,     7,  1895,\n",
      "              4, 50118,   771,    35,    38,  4443,    98,     4,    20,     2,\n",
      "              2,  2264,    16,     5,  1650,   101,   116,    85,    18,  2779,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,    38,   206,    24,    18,   164,     7,  1895,\n",
      "              4, 50118,   771,    35,    38,  4443,    98,     4,    20,     2,\n",
      "              2,  2264,    16,     5,  1650,   101,   116,    85,    18,  5419,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   771,    35,  7204,  8429,     4,  2615,    38,   244,    47,\n",
      "            116, 50118,   448,    35,  8976,     6, 20280,     6,    42,     2,\n",
      "              2, 13841,    32,     5,    80,  6864,   116,    96,    10,  2391,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,  7204,  8429,     4,  2615,    38,   244,    47,\n",
      "            116, 50118,   448,    35,  8976,     6, 20280,     6,    42,     2,\n",
      "              2, 13841,    32,     5,    80,  6864,   116,    96,    10,  2303,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,  7204,  8429,     4,  2615,    38,   244,    47,\n",
      "            116, 50118,   448,    35,  8976,     6, 20280,     6,    42,     2,\n",
      "              2, 13841,    32,     5,    80,  6864,   116,    96,     5,  2014,\n",
      "              4,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0,   448,    35,   520,   222,    47,    78,   465,     5,  1883,\n",
      "           3187,     8,   383,  1716,   116, 50118,   771,    35,   572,     2,\n",
      "              2,  2264,    32,    51,  1686,    59,   116,    83,  1703,  3213,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,   520,   222,    47,    78,   465,     5,  1883,\n",
      "           3187,     8,   383,  1716,   116, 50118,   771,    35,   572,    38,\n",
      "              2,     2,  2264,    32,    51,  1686,    59,   116,    83,   668,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,   520,   222,    47,    78,   465,     5,  1883,\n",
      "           3187,     8,   383,  1716,   116, 50118,   771,    35,   572,    38,\n",
      "              2,     2,  2264,    32,    51,  1686,    59,   116,    83,  1846,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   597,    35,    20, 17299,    40,   386,    23,   130,    42,\n",
      "           1390,     6,    16,    14,   235,     2,     2,  2264,   109,    47,\n",
      "            216,    59,     5, 17299,   116,    85,    40,   386,    23,   130,\n",
      "             42,     2],\n",
      "         [    0,   597,    35,    20, 17299,    40,   386,    23,   130,    42,\n",
      "           1390,     6,    16,    14,   235,     2,     2,  2264,   109,    47,\n",
      "            216,    59,     5, 17299,   116,    85,    40,   386,    23,   130,\n",
      "           3859,     2],\n",
      "         [    0,   597,    35,    20, 17299,    40,   386,    23,   130,    42,\n",
      "           1390,     6,    16,    14,   235,     2,     2,  2264,   109,    47,\n",
      "            216,    59,     5, 17299,   116,    85,    40,   386,    23,   799,\n",
      "           3859,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,   448,    35,   520,   222,    47,    78,   465,     5,  1883,\n",
      "           3187,     8,   383,  1716,   116, 50118,   771,    35,   572,     2,\n",
      "              2,  2264,    32,    51,  1686,    59,   116,    83,  1703,  3213,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,   520,   222,    47,    78,   465,     5,  1883,\n",
      "           3187,     8,   383,  1716,   116, 50118,   771,    35,   572,    38,\n",
      "              2,     2,  2264,    32,    51,  1686,    59,   116,    83,   668,\n",
      "              4,     2],\n",
      "         [    0,   448,    35,   520,   222,    47,    78,   465,     5,  1883,\n",
      "           3187,     8,   383,  1716,   116, 50118,   771,    35,   572,    38,\n",
      "              2,     2,  2264,    32,    51,  1686,    59,   116,    83,  1846,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   597,    35,    20, 17299,    40,   386,    23,   130,    42,\n",
      "           1390,     6,    16,    14,   235,     2,     2,  2264,   109,    47,\n",
      "            216,    59,     5, 17299,   116,    85,    40,   386,    23,   130,\n",
      "             42,     2],\n",
      "         [    0,   597,    35,    20, 17299,    40,   386,    23,   130,    42,\n",
      "           1390,     6,    16,    14,   235,     2,     2,  2264,   109,    47,\n",
      "            216,    59,     5, 17299,   116,    85,    40,   386,    23,   130,\n",
      "           3859,     2],\n",
      "         [    0,   597,    35,    20, 17299,    40,   386,    23,   130,    42,\n",
      "           1390,     6,    16,    14,   235,     2,     2,  2264,   109,    47,\n",
      "            216,    59,     5, 17299,   116,    85,    40,   386,    23,   799,\n",
      "           3859,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "{'input_ids': tensor([[[    0,   771,    35,    38,  2813,    38,  1467,     5,   498,     9,\n",
      "              5,  7717,     7,   928,     4,   125,     2,     2,  2264,    16,\n",
      "              5,   313,   164,     7,   109,   116,  2381,    15,     5,  3742,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,    38,  2813,    38,  1467,     5,   498,     9,\n",
      "              5,  7717,     7,   928,     4,   125,     2,     2,  2264,    16,\n",
      "              5,   313,   164,     7,   109,   116,  5293,    10,  1028,   486,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,    38,  2813,    38,  1467,     5,   498,     9,\n",
      "              5,  7717,     7,   928,     4,   125,     2,     2,  2264,    16,\n",
      "              5,   313,   164,     7,   109,   116,  4624,    10,  2341,  1805,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   448,    35, 11258,    47,  1004,   160,     5,  8054,  6700,\n",
      "            116, 50118,   771,    35, 12191,     6,    53,    47,    26,     2,\n",
      "              2, 17485,     5,   693,  1004,   160,     5,  6700,   116, 20240,\n",
      "              4,     2],\n",
      "         [    0,   448,    35, 11258,    47,  1004,   160,     5,  8054,  6700,\n",
      "            116, 50118,   771,    35, 12191,     6,    53,    47,    26,     2,\n",
      "              2, 17485,     5,   693,  1004,   160,     5,  6700,   116,  5359,\n",
      "              4,     2],\n",
      "         [    0,   448,    35, 11258,    47,  1004,   160,     5,  8054,  6700,\n",
      "            116, 50118,   771,    35, 12191,     6,    53,    47,    26,     2,\n",
      "              2, 17485,     5,   693,  1004,   160,     5,  6700,   116,   440,\n",
      "              4,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[[    0,   771,    35,    38,  2813,    38,  1467,     5,   498,     9,\n",
      "              5,  7717,     7,   928,     4,   125,     2,     2,  2264,    16,\n",
      "              5,   313,   164,     7,   109,   116,  2381,    15,     5,  3742,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,    38,  2813,    38,  1467,     5,   498,     9,\n",
      "              5,  7717,     7,   928,     4,   125,     2,     2,  2264,    16,\n",
      "              5,   313,   164,     7,   109,   116,  5293,    10,  1028,   486,\n",
      "              4,     2],\n",
      "         [    0,   771,    35,    38,  2813,    38,  1467,     5,   498,     9,\n",
      "              5,  7717,     7,   928,     4,   125,     2,     2,  2264,    16,\n",
      "              5,   313,   164,     7,   109,   116,  4624,    10,  2341,  1805,\n",
      "              4,     2]],\n",
      "\n",
      "        [[    0,   448,    35, 11258,    47,  1004,   160,     5,  8054,  6700,\n",
      "            116, 50118,   771,    35, 12191,     6,    53,    47,    26,     2,\n",
      "              2, 17485,     5,   693,  1004,   160,     5,  6700,   116, 20240,\n",
      "              4,     2],\n",
      "         [    0,   448,    35, 11258,    47,  1004,   160,     5,  8054,  6700,\n",
      "            116, 50118,   771,    35, 12191,     6,    53,    47,    26,     2,\n",
      "              2, 17485,     5,   693,  1004,   160,     5,  6700,   116,  5359,\n",
      "              4,     2],\n",
      "         [    0,   448,    35, 11258,    47,  1004,   160,     5,  8054,  6700,\n",
      "            116, 50118,   771,    35, 12191,     6,    53,    47,    26,     2,\n",
      "              2, 17485,     5,   693,  1004,   160,     5,  6700,   116,   440,\n",
      "              4,     2]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1]]])}\n",
      "################################\n",
      "_test_squad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 20:54:09,549 | base.py | INFO | Check data directory: D:\\resource\\data\\HotpotQA\n",
      "2024-10-08 20:54:09,549 | base.py | INFO | √ ./hotpot_dev_distractor_v1.json\n",
      "2024-10-08 20:54:09,549 | base.py | INFO | √ ./hotpot_dev_fullwiki_v1.json\n",
      "2024-10-08 20:54:09,549 | base.py | INFO | √ ./hotpot_test_fullwiki_v1.json\n",
      "2024-10-08 20:54:09,555 | base.py | INFO | √ ./hotpot_train_v1.1.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 32251,  1485,   165,  4625,     5,  9601,    23,  1582,  2616,\n",
      "           654,   116,     2,     2, 16713,  1215,   387, 20734,  1215,  1096,\n",
      "         50118, 16713,  2616,   654,    21,    41,   470,  1037,   177,     7,\n",
      "          3094,     2],\n",
      "        [    0, 32251,  1485,   165,  4625,     5, 11119,    23,  1582,  2616,\n",
      "           654,   116,     2,     2, 16713,  1215,   387, 20734,  1215,  1096,\n",
      "         50118, 16713,  2616,   654,    21,    41,   470,  1037,   177,     7,\n",
      "          3094,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[    0, 32251,  1485,   165,  4625,     5,  9601,    23,  1582,  2616,\n",
      "           654,   116,     2,     2, 16713,  1215,   387, 20734,  1215,  1096,\n",
      "         50118, 16713,  2616,   654,    21,    41,   470,  1037,   177,     7,\n",
      "          3094,     2],\n",
      "        [    0, 32251,  1485,   165,  4625,     5, 11119,    23,  1582,  2616,\n",
      "           654,   116,     2,     2, 16713,  1215,   387, 20734,  1215,  1096,\n",
      "         50118, 16713,  2616,   654,    21,    41,   470,  1037,   177,     7,\n",
      "          3094,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "################################\n",
      "{'input_ids': tensor([[    0, 13841,   222,  1582,  2616,   654,   185,   317,   116,     2,\n",
      "             2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,\n",
      "           654,    21,    41,   470,  1037,   177,     7,  3094,     5,  2234,\n",
      "             9,     2],\n",
      "        [    0, 32251,  1485,   165,   351,  1582,  2616,   654,   116,     2,\n",
      "             2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,\n",
      "           654,    21,    41,   470,  1037,   177,     7,  3094,     5,  2234,\n",
      "             9,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[    0, 13841,   222,  1582,  2616,   654,   185,   317,   116,     2,\n",
      "             2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,\n",
      "           654,    21,    41,   470,  1037,   177,     7,  3094,     5,  2234,\n",
      "             9,     2],\n",
      "        [    0, 32251,  1485,   165,   351,  1582,  2616,   654,   116,     2,\n",
      "             2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,\n",
      "           654,    21,    41,   470,  1037,   177,     7,  3094,     5,  2234,\n",
      "             9,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "################################\n",
      "{'input_ids': tensor([[    0,  2264,  3195,    21,   341,     7, 20251,     5,   654,   212,\n",
      "          4038,     9,     5,  1582,  2616,     2,     2, 16713,  1215,   387,\n",
      "         20734,  1215,  1096, 50118, 16713,  2616,   654,    21,    41,   470,\n",
      "          1037,     2],\n",
      "        [    0,  2264,    21,     5,  4782,     9,  1582,  2616,   654,   116,\n",
      "             2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,\n",
      "          2616,   654,    21,    41,   470,  1037,   177,     7,  3094,     5,\n",
      "          2234,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[    0,  2264,  3195,    21,   341,     7, 20251,     5,   654,   212,\n",
      "          4038,     9,     5,  1582,  2616,     2,     2, 16713,  1215,   387,\n",
      "         20734,  1215,  1096, 50118, 16713,  2616,   654,    21,    41,   470,\n",
      "          1037,     2],\n",
      "        [    0,  2264,    21,     5,  4782,     9,  1582,  2616,   654,   116,\n",
      "             2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,\n",
      "          2616,   654,    21,    41,   470,  1037,   177,     7,  3094,     5,\n",
      "          2234,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "################################\n",
      "{'input_ids': tensor([[    0,  2264,   183,    21,     5,   177,   702,    15,   116,     2,\n",
      "             2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,\n",
      "           654,    21,    41,   470,  1037,   177,     7,  3094,     5,  2234,\n",
      "             9,     2],\n",
      "        [    0,  2264,    16,     5,  9601,   765,    13,   116,     2,     2,\n",
      "         16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,   654,\n",
      "            21,    41,   470,  1037,   177,     7,  3094,     5,  2234,     9,\n",
      "             5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[    0,  2264,   183,    21,     5,   177,   702,    15,   116,     2,\n",
      "             2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,\n",
      "           654,    21,    41,   470,  1037,   177,     7,  3094,     5,  2234,\n",
      "             9,     2],\n",
      "        [    0,  2264,    16,     5,  9601,   765,    13,   116,     2,     2,\n",
      "         16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,   654,\n",
      "            21,    41,   470,  1037,   177,     7,  3094,     5,  2234,     9,\n",
      "             5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "################################\n",
      "{'input_ids': tensor([[    0,  2264,    21,     5,  4782,     9,  1582,  2616,   654,   116,\n",
      "             2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,\n",
      "          2616,   654,    21,    41,   470,  1037,   177,     7,  3094,     5,\n",
      "          2234,     2],\n",
      "        [    0,  2264,   473,  9601,  1413,    13,   116,     2,     2, 16713,\n",
      "          1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,   654,    21,\n",
      "            41,   470,  1037,   177,     7,  3094,     5,  2234,     9,     5,\n",
      "           496,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[    0,  2264,    21,     5,  4782,     9,  1582,  2616,   654,   116,\n",
      "             2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,\n",
      "          2616,   654,    21,    41,   470,  1037,   177,     7,  3094,     5,\n",
      "          2234,     2],\n",
      "        [    0,  2264,   473,  9601,  1413,    13,   116,     2,     2, 16713,\n",
      "          1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,   654,    21,\n",
      "            41,   470,  1037,   177,     7,  3094,     5,  2234,     9,     5,\n",
      "           496,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "################################\n",
      "{'input_ids': tensor([[    0,  2264,   183,    21,     5,  1582,  2616,   702,    15,   116,\n",
      "             2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,\n",
      "          2616,   654,    21,    41,   470,  1037,   177,     7,  3094,     5,\n",
      "          2234,     2],\n",
      "        [    0, 12375,   351,  1582,  2616,   654,   116,     2,     2, 16713,\n",
      "          1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,   654,    21,\n",
      "            41,   470,  1037,   177,     7,  3094,     5,  2234,     9,     5,\n",
      "           496,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[    0,  2264,   183,    21,     5,  1582,  2616,   702,    15,   116,\n",
      "             2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118, 16713,\n",
      "          2616,   654,    21,    41,   470,  1037,   177,     7,  3094,     5,\n",
      "          2234,     2],\n",
      "        [    0, 12375,   351,  1582,  2616,   654,   116,     2,     2, 16713,\n",
      "          1215,   387, 20734,  1215,  1096, 50118, 16713,  2616,   654,    21,\n",
      "            41,   470,  1037,   177,     7,  3094,     5,  2234,     9,     5,\n",
      "           496,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "################################\n",
      "{'input_ids': tensor([[    0,  2264,  5584,   222,  1582,  2616,   654,   185,   317,    11,\n",
      "           116,     2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118,\n",
      "         16713,  2616,   654,    21,    41,   470,  1037,   177,     7,  3094,\n",
      "             5,     2],\n",
      "        [    0,  2264,   343,   222,  1582,  2616,   654,   185,   317,    11,\n",
      "           116,     2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118,\n",
      "         16713,  2616,   654,    21,    41,   470,  1037,   177,     7,  3094,\n",
      "             5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[    0,  2264,  5584,   222,  1582,  2616,   654,   185,   317,    11,\n",
      "           116,     2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118,\n",
      "         16713,  2616,   654,    21,    41,   470,  1037,   177,     7,  3094,\n",
      "             5,     2],\n",
      "        [    0,  2264,   343,   222,  1582,  2616,   654,   185,   317,    11,\n",
      "           116,     2,     2, 16713,  1215,   387, 20734,  1215,  1096, 50118,\n",
      "         16713,  2616,   654,    21,    41,   470,  1037,   177,     7,  3094,\n",
      "             5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "################################\n",
      "_test_hotpotqa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[64790, 64792,   809,  ...,   267,  3764, 30953],\n",
      "        [64790, 64792,   809,  ...,    13,  1036,  1147]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'position_ids': tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
      "        [  0,   1,   2,  ..., 509, 510, 511]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939],\n",
      "        [64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]])}\n",
      "################################\n",
      "{'input_ids': tensor([[64790, 64792,   809,  ...,  1669,   290,   709],\n",
      "        [64790, 64792,   809,  ..., 30945,   428, 30910]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'position_ids': tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
      "        [  0,   1,   2,  ..., 509, 510, 511]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939],\n",
      "        [64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]])}\n",
      "################################\n",
      "{'input_ids': tensor([[64790, 64792,   809,  ...,   910, 11028,  2575],\n",
      "        [64790, 64792,   809,  ...,   314, 30967, 30937]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'position_ids': tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
      "        [  0,   1,   2,  ..., 509, 510, 511]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939],\n",
      "        [64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]])}\n",
      "################################\n",
      "{'input_ids': tensor([[64790, 64792,   809,  ...,   323,   284,  1603],\n",
      "        [64790, 64792,   809,  ...,  9781,   649,    13]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'position_ids': tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
      "        [  0,   1,   2,  ..., 509, 510, 511]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939],\n",
      "        [64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]])}\n",
      "################################\n",
      "{'input_ids': tensor([[64790, 64792,   809,  ..., 30943, 30940, 30939],\n",
      "        [64790, 64792,   809,  ...,    13,   353,  2310]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'position_ids': tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
      "        [  0,   1,   2,  ..., 509, 510, 511]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939],\n",
      "        [64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]])}\n",
      "################################\n",
      "{'input_ids': tensor([[64790, 64792,   809,  ..., 30932, 11057,   293],\n",
      "        [64790, 64792,   809,  ..., 30913,   293,   319]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'position_ids': tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
      "        [  0,   1,   2,  ..., 509, 510, 511]])}\n",
      "--------------------------------\n",
      "{'input_ids': tensor([[64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939],\n",
      "        [64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]])}\n",
      "################################\n",
      "{'input_ids': tensor([[64790, 64792,   809,  ..., 30932,   473,   394],\n",
      "        [64790, 64792,   809,  ...,   267,   550,  1109]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'position_ids': tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
      "        [  0,   1,   2,  ..., 509, 510, 511]])}\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939],\n",
      "        [64790, 64792,   809,   383,   260,  7486, 30932,   344,   720,   289,\n",
      "           950,   267,  1845,  4177,  7724,   293,  7511,   267,  3238,   289,\n",
      "           267,  2021,  3040, 30930,    13,   986,  8192, 30954,    13, 11355,\n",
      "         30910, 30939]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]])}\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "def test_generate_model_inputs():\n",
    "\n",
    "    def _test_race():\n",
    "        print(_test_race.__name__)\n",
    "        data_dir = DATA_SUMMARY[RaceDataset.dataset_name][\"path\"]\n",
    "        model_path = MODEL_SUMMARY[RobertaLargeFinetunedRace.model_name][\"path\"]\n",
    "        # model_path = MODEL_SUMMARY[LongformerLarge4096AnsweringRace.model_name][\"path\"]\n",
    "        dataset = RaceDataset(data_dir)\n",
    "        model = RobertaLargeFinetunedRace(model_path, device=\"cpu\")\n",
    "        # model = LongformerLarge4096AnsweringRace(model_path, device=\"cpu\")\n",
    "\n",
    "        for i, batch in enumerate(dataset.yield_batch(batch_size=2, types=[\"train\", \"dev\"], difficulties=[\"high\"])):\n",
    "            model_inputs = RaceDataset.generate_model_inputs(batch, model.tokenizer, model.model_name, max_length=32)\n",
    "            print(model_inputs)\n",
    "            print('-' * 32)\n",
    "            model_inputs = model.generate_model_inputs(batch, max_length=32)\n",
    "            print(model_inputs)\n",
    "            print('#' * 32)\n",
    "            if i > 5:\n",
    "                break\n",
    "\n",
    "    def _test_dream():\n",
    "        print(_test_dream.__name__)\n",
    "        data_dir = DATA_SUMMARY[DreamDataset.dataset_name][\"path\"] \n",
    "        model_path = MODEL_SUMMARY[RobertaLargeFinetunedRace.model_name][\"path\"]\n",
    "        dataset = DreamDataset(data_dir)\n",
    "        model = RobertaLargeFinetunedRace(model_path, device=\"cpu\")\n",
    "        for i, batch in enumerate(dataset.yield_batch(batch_size=2, types=[\"train\", \"dev\"])):\n",
    "            model_inputs = DreamDataset.generate_model_inputs(batch, model.tokenizer, model.model_name, max_length=32)\n",
    "            print(model_inputs)\n",
    "            print('-' * 32)\n",
    "            model_inputs = model.generate_model_inputs(batch, max_length=32)\n",
    "            print(model_inputs)\n",
    "            print('#' * 32)\n",
    "            if i > 5:\n",
    "                break\n",
    "\n",
    "    def _test_squad():\n",
    "        print(_test_squad.__name__)\n",
    "        data_dir = DATA_SUMMARY[SquadDataset.dataset_name][\"path\"]\n",
    "        model_path = MODEL_SUMMARY[RobertaBaseSquad2.model_name][\"path\"]\n",
    "        dataset = SquadDataset(data_dir)\n",
    "        model = RobertaBaseSquad2(model_path, device=\"cpu\")\n",
    "\n",
    "        for i, batch in enumerate(dataset.yield_batch(batch_size=2, type_=\"dev\", version=\"1.1\")):\n",
    "            model_inputs = SquadDataset.generate_model_inputs(batch, model.tokenizer, model.model_name, max_length=32)\n",
    "            print(model_inputs)\n",
    "            print('-' * 32)\n",
    "            model_inputs = model.generate_model_inputs(batch, max_length=32)\n",
    "            print(model_inputs)\n",
    "            print('#' * 32)\n",
    "            if i > 5:\n",
    "                break\n",
    "\n",
    "    def _test_hotpotqa():\n",
    "        print(_test_hotpotqa.__name__)\n",
    "        data_dir = DATA_SUMMARY[HotpotqaDataset.dataset_name][\"path\"]\n",
    "        model_path = MODEL_SUMMARY[Chatglm26bInt4.model_name][\"path\"]\n",
    "        dataset = HotpotqaDataset(data_dir)\n",
    "        model = Chatglm6bInt4(model_path, device=\"cuda\")\n",
    "        for i, batch in enumerate(dataset.yield_batch(batch_size=2, filename=\"dev_distractor_v1.json\")):\n",
    "            model_inputs = HotpotqaDataset.generate_model_inputs(batch, model.tokenizer, model.model_name, max_length=512)\n",
    "            print(model_inputs)\n",
    "            print('-' * 32)\n",
    "            model_inputs = model.generate_model_inputs(batch, max_length=32)\n",
    "            print(model_inputs)\n",
    "            print('#' * 32)\n",
    "            if i > 5:\n",
    "                break\t\t\n",
    "\n",
    "    logger = initialize_logger(os.path.join(LOG_DIR, \"sanity.log\"), 'w')\n",
    "    _test_race()\n",
    "    _test_dream()\n",
    "    _test_squad()\n",
    "    _test_hotpotqa()\n",
    "    terminate_logger(logger)\n",
    "\n",
    "test_generate_model_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试pipeline.easy_inference_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_pipeline():\n",
    "\n",
    "    def _test_race():\n",
    "        race_pipeline = RacePipeline()\n",
    "        pipeline = race_pipeline.easy_inference_pipeline(\n",
    "            dataset_class_name = \"RaceDataset\",\n",
    "            model_class_name = \"RobertaLargeFinetunedRace\",\n",
    "            batch_size = 2,\n",
    "            dataset_kwargs = {\"types\": [\"train\"], \"difficulties\": [\"high\", \"middle\"]},\n",
    "            model_kwargs = {\"max_length\": 512},\n",
    "        )\n",
    "\n",
    "    def _test_squad():\n",
    "        squad_pipeline = SquadPipeline()\n",
    "        pipeline = squad_pipeline.easy_inference_pipeline(\n",
    "            dataset_class_name = \"SquadDataset\",\n",
    "            model_class_name = \"RobertaBaseSquad2\",\n",
    "            batch_size = 2,\n",
    "            dataset_kwargs = {\"type_\": \"train\", \"version\": \"2.0\"},\n",
    "            model_kwargs = {\"max_length\": 512},\n",
    "        )\n",
    "\n",
    "    # logger = initialize_logger(os.path.join(LOG_DIR, \"sanity.log\"), 'w')\n",
    "    _test_race()\n",
    "    # _test_squad()\n",
    "    # terminate_logger(logger)\n",
    "    \n",
    "    \n",
    "test_inference_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T12:46:06.345219Z",
     "start_time": "2024-10-11T12:46:06.334827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: -- [-h] [--device DEVICE] [--default_pretrained_model DEFAULT_PRETRAINED_MODEL]\n",
      "          [--default_wordvector DEFAULT_WORDVECTOR] [--default_embedding_size DEFAULT_EMBEDDING_SIZE]\n",
      "          [--word_tokenizer_source WORD_TOKENIZER_SOURCE] [--sentence_tokenizer_source SENTENCE_TOKENIZER_SOURCE]\n",
      "          [--parse_tree_parser_source PARSE_TREE_PARSER_SOURCE] [--dependency_parser_source DEPENDENCY_PARSER_SOURCE]\n",
      "          [--max_article_token MAX_ARTICLE_TOKEN] [--max_article_sentence MAX_ARTICLE_SENTENCE]\n",
      "          [--max_article_sentence_token MAX_ARTICLE_SENTENCE_TOKEN] [--max_question_token MAX_QUESTION_TOKEN]\n",
      "          [--max_option_token MAX_OPTION_TOKEN] [--n_choices N_CHOICES] [--multi_choice MULTI_CHOICE]\n",
      "          [--test_while_train TEST_WHILE_TRAIN] [--comatch_wordvector COMATCH_WORDVECTOR]\n",
      "          [--dcmn_pretrained_model DCMN_PRETRAINED_MODEL] [--duma_pretrained_model DUMA_PRETRAINED_MODEL]\n",
      "          [--hrca_pretrained_model HRCA_PRETRAINED_MODEL] [--use_pretrained_model USE_PRETRAINED_MODEL]\n",
      "          [--load_pretrained_model_in_module LOAD_PRETRAINED_MODEL_IN_MODULE]\n",
      "          [--pretrained_model_device PRETRAINED_MODEL_DEVICE] [--n_epochs N_EPOCHS] [--optimizer OPTIMIZER]\n",
      "          [--learning_rate LEARNING_RATE] [--weight_decay WEIGHT_DECAY] [--lr_step_size LR_STEP_SIZE]\n",
      "          [--lr_multiplier LR_MULTIPLIER] [--save_checkpoint_per_epoch SAVE_CHECKPOINT_PER_EPOCH]\n",
      "          [--comatch_embedding_size COMATCH_EMBEDDING_SIZE] [--comatch_bilstm_hidden_size COMATCH_BILSTM_HIDDEN_SIZE]\n",
      "          [--comatch_bilstm_num_layers COMATCH_BILSTM_NUM_LAYERS]\n",
      "          [--comatch_bilstm_bidirectional COMATCH_BILSTM_BIDIRECTIONAL]\n",
      "          [--comatch_bilstm_dropout COMATCH_BILSTM_DROPOUT] [--dcmn_scoring_method DCMN_SCORING_METHOD]\n",
      "          [--dcmn_num_passage_sentence_selection DCMN_NUM_PASSAGE_SENTENCE_SELECTION]\n",
      "          [--dcmn_encoding_size DCMN_ENCODING_SIZE] [--duma_num_layers DUMA_NUM_LAYERS]\n",
      "          [--duma_encoding_size DUMA_ENCODING_SIZE] [--duma_fuse_method DUMA_FUSE_METHOD]\n",
      "          [--duma_mha_num_heads DUMA_MHA_NUM_HEADS] [--duma_mha_dropout_rate DUMA_MHA_DROPOUT_RATE]\n",
      "          [--hrca_num_layers HRCA_NUM_LAYERS] [--hrca_encoding_size HRCA_ENCODING_SIZE]\n",
      "          [--hrca_fuse_method HRCA_FUSE_METHOD] [--hrca_mha_num_heads HRCA_MHA_NUM_HEADS]\n",
      "          [--hrca_mha_dropout_rate HRCA_MHA_DROPOUT_RATE] [--hrca_plus HRCA_PLUS]\n",
      "--: error: unrecognized arguments: -f C:\\Users\\caoyang\\AppData\\Roaming\\jupyter\\runtime\\kernel-12f0f996-2729-420d-b08e-1058321d56c9.json\n"
     ]
    }
   ],
   "source": [
    "args = load_args(Config = ModuleConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:07:20.839689Z",
     "start_time": "2024-10-11T13:07:20.819077Z"
    }
   },
   "outputs": [],
   "source": [
    "args.device = \"cpu\"\n",
    "args.comatch_embedding_size = 32\n",
    "args.comatch_bilstm_hidden_size = 16\n",
    "comatch = CoMatch(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:07:22.241700Z",
     "start_time": "2024-10-11T13:07:22.236678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoMatch(\n",
       "  (Encoder_P): MaskedLSTM(\n",
       "    (lstm): LSTM(300, 75, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (Encoder_Q): MaskedLSTM(\n",
       "    (lstm): LSTM(300, 75, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (Encoder_A): MaskedLSTM(\n",
       "    (lstm): LSTM(300, 75, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (Encoder_C): LSTM(300, 75, batch_first=True, bidirectional=True)\n",
       "  (Encoder_H_s): LSTM(150, 75, batch_first=True, bidirectional=True)\n",
       "  (W_g): Linear(in_features=150, out_features=150, bias=True)\n",
       "  (W_m): Linear(in_features=300, out_features=150, bias=True)\n",
       "  (w): Linear(in_features=150, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:07:24.258023Z",
     "start_time": "2024-10-11T13:07:24.243182Z"
    }
   },
   "outputs": [],
   "source": [
    "N_CHOICES = 4\n",
    "batch_size = 32\n",
    "test_input = {\n",
    "    'P': torch.FloatTensor(batch_size, args.max_article_sentence, args.max_article_sentence_token, args.comatch_embedding_size),\n",
    "    'Q': torch.FloatTensor(batch_size, args.max_question_token, args.comatch_embedding_size),\n",
    "    'A': torch.FloatTensor(batch_size, N_CHOICES, args.max_option_token, args.comatch_embedding_size),\n",
    "    'P_shape': torch.LongTensor([[100] * args.max_article_sentence] * batch_size),\n",
    "    'Q_shape': torch.LongTensor([100] * batch_size),\n",
    "    'A_shape': torch.LongTensor([[100] * N_CHOICES] * batch_size),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:07:27.023735Z",
     "start_time": "2024-10-11T13:07:25.700270Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 11769413632 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m logit_output \u001b[38;5;241m=\u001b[39m comatch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtest_input)\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\py39-adapters\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\py39-adapters\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\code\\python\\project\\caoyang\\project_019_llm_reasoning\\easyqa\\src\\modules\\comatch_module.py:230\u001b[0m, in \u001b[0;36mCoMatch.forward\u001b[1;34m(self, P, Q, A, P_shape, Q_shape, A_shape, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, P, Q, A, P_shape, Q_shape, A_shape, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 230\u001b[0m \tH_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncoder_P\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u001b[38;5;66;03m# H_p\t\t\t\t\t\t: (batch_size * max_article_sentence, max_article_sentence_token, comatch_bilstm_hidden_size)\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \tH_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncoder_Q(Q, Q_shape)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u001b[38;5;66;03m# H_q\t\t\t\t\t\t: (batch_size, max_question_token, comatch_bilstm_hidden_size)\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \tH_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncoder_A(A\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md), A_shape\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u001b[38;5;66;03m# H_a\t\t\t\t\t\t: (batch_size * N_CHOICES, max_option_token, comatch_bilstm_hidden_size)\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\py39-adapters\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\py39-adapters\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\code\\python\\project\\caoyang\\project_019_llm_reasoning\\easyqa\\src\\modules\\easy_module.py:55\u001b[0m, in \u001b[0;36mMaskedLSTM.forward\u001b[1;34m(self, x, x_shape)\u001b[0m\n\u001b[0;32m     53\u001b[0m x_masked, x_mask \u001b[38;5;241m=\u001b[39m mask(x, x_shape)\n\u001b[0;32m     54\u001b[0m x_drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x_masked)\n\u001b[1;32m---> 55\u001b[0m x_hidden, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_drop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m x_hidden_masked, x_hidden_mask \u001b[38;5;241m=\u001b[39m mask(x_hidden, x_shape)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_hidden_masked\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\py39-adapters\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\py39-adapters\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\py39-adapters\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    915\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 11769413632 bytes."
     ]
    }
   ],
   "source": [
    "logit_output = comatch(**test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-adapters-jupyter",
   "language": "python",
   "name": "py39-adapters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
